{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUcVBSX1wktXkjaDiWJnus",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Misha-private/Demo-repo/blob/main/MGBF_NN0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQKNvH6kyMMb",
        "outputId": "08ccafa8-ca17-484a-8a3e-e85e544cd2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter, MSE = 0 ... 0.14164191977885301\n",
            "iter, MSE = 1000 ... 0.0003143098406446609\n",
            "iter, MSE = 2000 ... 0.00023133009234730116\n",
            "iter, MSE = 3000 ... 0.0002002350757830773\n",
            "iter, MSE = 4000 ... 0.00018651462274770877\n",
            "iter, MSE = 5000 ... 0.0001783485696458777\n",
            "iter, MSE = 6000 ... 0.00017181920145841874\n",
            "iter, MSE = 7000 ... 0.0001656470108082495\n",
            "iter, MSE = 8000 ... 0.00015930902537419342\n",
            "iter, MSE = 9000 ... 0.00015252952672984937\n",
            "iter, MSE = 10000 ... 0.00014513751219525587\n",
            "iter, MSE = 11000 ... 0.0001370307000006144\n",
            "iter, MSE = 12000 ... 0.0001281781365323614\n",
            "iter, MSE = 13000 ... 0.0001186361399113392\n",
            "iter, MSE = 14000 ... 0.00010856027948504874\n",
            "iter, MSE = 15000 ... 9.819861009702242e-05\n",
            "iter, MSE = 16000 ... 8.785910150687029e-05\n",
            "iter, MSE = 17000 ... 7.78578233182224e-05\n",
            "iter, MSE = 18000 ... 6.846585521473941e-05\n",
            "iter, MSE = 19000 ... 5.9873402398506735e-05\n",
            "iter, MSE = 20000 ... 5.217948087445581e-05\n",
            "iter, MSE = 21000 ... 4.5403294535568094e-05\n",
            "iter, MSE = 22000 ... 3.9506817611639787e-05\n",
            "iter, MSE = 23000 ... 3.441849946591384e-05\n",
            "iter, MSE = 24000 ... 3.005215828333285e-05\n",
            "iter, MSE = 25000 ... 2.6319383559291272e-05\n",
            "iter, MSE = 26000 ... 2.313635413843177e-05\n",
            "iter, MSE = 27000 ... 2.0426855237581388e-05\n",
            "iter, MSE = 28000 ... 1.8123128232854964e-05\n",
            "iter, MSE = 29000 ... 1.6165675947328646e-05\n",
            "iter, MSE = 30000 ... 1.450265693739869e-05\n",
            "iter, MSE = 31000 ... 1.3089167505268514e-05\n",
            "iter, MSE = 32000 ... 1.1886526102732577e-05\n",
            "iter, MSE = 33000 ... 1.0861592578353843e-05\n",
            "iter, MSE = 34000 ... 9.98612697609464e-06\n",
            "iter, MSE = 35000 ... 9.236187844222474e-06\n",
            "iter, MSE = 36000 ... 8.591571818530724e-06\n",
            "iter, MSE = 37000 ... 8.035297954812558e-06\n",
            "iter, MSE = 38000 ... 7.553140275050266e-06\n",
            "iter, MSE = 39000 ... 7.133210613029529e-06\n",
            "iter, MSE = 40000 ... 6.765591893004858e-06\n",
            "iter, MSE = 41000 ... 6.442020086580686e-06\n",
            "iter, MSE = 42000 ... 6.155611616046103e-06\n",
            "iter, MSE = 43000 ... 5.900632017782918e-06\n",
            "iter, MSE = 44000 ... 5.6723012132864e-06\n",
            "iter, MSE = 45000 ... 5.466630657766427e-06\n",
            "iter, MSE = 46000 ... 5.2802878316516975e-06\n",
            "iter, MSE = 47000 ... 5.110483904255081e-06\n",
            "iter, MSE = 48000 ... 4.954880848964398e-06\n",
            "iter, MSE = 49000 ... 4.8115147664701e-06\n",
            "iter, MSE = 50000 ... 4.678732637947664e-06\n",
            "iter, MSE = 51000 ... 4.555140160850653e-06\n",
            "iter, MSE = 52000 ... 4.439558704532967e-06\n",
            "iter, MSE = 53000 ... 4.330989757486689e-06\n",
            "iter, MSE = 54000 ... 4.228585523522435e-06\n",
            "iter, MSE = 55000 ... 4.131624564427114e-06\n",
            "iter, MSE = 56000 ... 4.0394915865101525e-06\n",
            "iter, MSE = 57000 ... 3.951660633409236e-06\n",
            "iter, MSE = 58000 ... 3.867681082837598e-06\n",
            "iter, MSE = 59000 ... 3.78716595547332e-06\n"
          ]
        }
      ],
      "source": [
        "# More data\n",
        "\n",
        "import numpy as np\n",
        "from numpy import exp, array, random, dot\n",
        "\n",
        "\n",
        "class NeuronLayer():\n",
        "    def __init__(self, number_of_neurons, number_of_inputs_per_neuron):\n",
        "        self.synaptic_weights = 2 * random.random((number_of_inputs_per_neuron, number_of_neurons)) - 1\n",
        "\n",
        "\n",
        "class NeuralNetwork():\n",
        "    def __init__(self, layer1, layer2):\n",
        "        self.layer1 = layer1\n",
        "        self.layer2 = layer2\n",
        "\n",
        "    # The Sigmoid function, which describes an S shaped curve.\n",
        "    # We pass the weighted sum of the inputs through this function to\n",
        "    # normalise them between 0 and 1.\n",
        "    def __sigmoid(self, x):\n",
        "        return 1 / (1 + exp(-x))\n",
        "\n",
        "    # The derivative of the Sigmoid function.\n",
        "    # This is the gradient of the Sigmoid curve.\n",
        "    # It indicates how confident we are about the existing weight.\n",
        "    def __sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    # We train the neural network through a process of trial and error.\n",
        "    # Adjusting the synaptic weights each time.\n",
        "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations, iteration_increment):\n",
        "        for iteration in range(number_of_training_iterations):\n",
        "            # Pass the training set through our neural network\n",
        "            output_from_layer_1, output_from_layer_2 = self.think(training_set_inputs)\n",
        "\n",
        "            # Calculate the error for layer 2 (The difference between the desired output\n",
        "            # and the predicted output).\n",
        "            layer2_error = training_set_outputs  - output_from_layer_2\n",
        "            layer2_delta = layer2_error * self.__sigmoid_derivative(output_from_layer_2)\n",
        "\n",
        "            if iteration%iteration_increment==0 :\n",
        "              #print(\"Output from layer_2\", training_set_outputs, output_from_layer_2 )\n",
        "              #print(\"Iteration: \", iteration, )\n",
        "              #print(\"Layer 2 Mean Squared Error (MSE): \" )\n",
        "              mse = np.mean(layer2_error**2)\n",
        "              print(f\"iter, MSE = {iteration} ... {mse}\")\n",
        "\n",
        "              #print((layer2_error)**2)\n",
        "\n",
        "\n",
        "            # Calculate the error for layer 1 (By looking at the weights in layer 1,\n",
        "            # we can determine by how much layer 1 contributed to the error in layer 2).\n",
        "            layer1_error = layer2_delta.dot(self.layer2.synaptic_weights.T)\n",
        "            layer1_delta = layer1_error * self.__sigmoid_derivative(output_from_layer_1)\n",
        "\n",
        "            # Calculate how much to adjust the weights by\n",
        "            layer1_adjustment = training_set_inputs.T.dot(layer1_delta)\n",
        "            layer2_adjustment = output_from_layer_1.T.dot(layer2_delta)\n",
        "\n",
        "            # Adjust the weights.\n",
        "            self.layer1.synaptic_weights += layer1_adjustment\n",
        "            self.layer2.synaptic_weights += layer2_adjustment\n",
        "\n",
        "    # The neural network thinks.\n",
        "    def think(self, inputs):\n",
        "        output_from_layer1 = self.__sigmoid(dot(inputs, self.layer1.synaptic_weights))\n",
        "        output_from_layer2 = self.__sigmoid(dot(output_from_layer1, self.layer2.synaptic_weights))\n",
        "        return output_from_layer1, output_from_layer2\n",
        "\n",
        "    # The neural network prints its weights\n",
        "    def print_weights(self):\n",
        "        print (\"    Input layer (9 neurons each with 10 examples and 10 outputs)\")\n",
        "        print (\"    Layer 1 (`` neurons, each with 10 inputs): \")\n",
        "        print (self.layer1.synaptic_weights)\n",
        "        print (\"    Layer 2 (1 neuron, with 10 inputs):\")\n",
        "        print (self.layer2.synaptic_weights)\n",
        "\n",
        "#\n",
        "# Main program\n",
        "#\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #Seed the random number generator\n",
        "    random.seed(1)\n",
        "\n",
        "    # Create layer 1 (10 neurons, each with 9 inputs)\n",
        "    layer1 = NeuronLayer(10, 9)\n",
        "\n",
        "    # Create layer 2 (1 neuron with 10 inputs)\n",
        "    layer2 = NeuronLayer(1, 10)\n",
        "\n",
        "    # Combine the layers to create a neural network\n",
        "    neural_network = NeuralNetwork(layer1, layer2)\n",
        "\n",
        "    #print (\"Stage 1) Random starting synaptic weights: \")\n",
        "    #neural_network.print_weights()\n",
        "\n",
        "    # The training set. We have 10 examples, each consisting of 9 input values\n",
        "    # and 1 output value.\n",
        "    training_set_inputs = array([[0.009, 0.048, 0.285, 0.018, 0.097, 0.571, 0.027, 0.145, 0.856],\n",
        "     [0.112, 0.162, 0.014, 0.409, 0.458, 0.310, 0.902, 0.952, 0.804],\n",
        "     [0.007, 0.006, 0.012, 0.000, 0.154, 0.309, 0.000, 0.500, 1.000],\n",
        "     [0.010, 0.070, 0.099, 0.070, 0.140, 0.169, 0.099, 0.169, 0.198],\n",
        "     [0.198, 0.148, 0.049, 0.148, 0.123, 0.074, 0.049, 0.074, 0.123],\n",
        "     [0.013, 0.036, 0.013, 0.036, 0.099, 0.036, 0.013, 0.036, 0.013],\n",
        "     [0.099, 0.049, 0.020, 0.049, 0.033, 0.016, 0.020, 0.016, 0.011],\n",
        "     [0.700, 0.049, 0.198, 0.049, 0.074, 0.148, 0.079, 0.089, 0.119],\n",
        "     [0.044, 0.083, 0.090, 0.000, 0.166, 0.148, 0.180, 0.249, 0.269],\n",
        "     [0.494, 0.329, 0.055,0.329, 0.226, 0.040, 0.165, 0.119, 0.023] ])\n",
        "\n",
        "    training_set_outputs = array([[0.006, 0.285, 0.028, 0.076, 0.173, 0.060, 0.066, 0.03, 0.071, 0.384]]).T\n",
        "\n",
        "    # Train the neural network using the training set.\n",
        "    # Do it 60,000 times and make small adjustments each time.\n",
        "    neural_network.train(training_set_inputs, training_set_outputs, iteration_increment=1000, number_of_training_iterations=60000)\n",
        "    #print(\"Stage 2) New synaptic weights after training: \")\n",
        "    #neural_network.print_weights()\n",
        "\n",
        "    # Test the neural network with a new situation.\n",
        "    #print (\"Stage 3) Considering a new situation [0.296, 0.025, 0.506, 0.246, 0.042, 0.444, 0.216, 0.037] -> ?: \")\n",
        "    hidden_state, output = neural_network.think(array([0.296, 0.144, 0.025, 0.506, 0.246, 0.042, 0.444, 0.216, 0.037]))\n",
        "    #print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrpNWhbLZcN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNKWeOlrZdT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pc44qoj_ZdXQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}