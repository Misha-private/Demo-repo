{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdgJQW+9YEv3BnZ2JLOw51",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Misha-private/Demo-repo/blob/main/ML_PINN_GNN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uYWo1izBfuVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUx892eyo130"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, csv, traceback, random\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ================== I/O ==================\n",
        "OUTDIR = \"outputs_PINN_Cgrid_anchor_colloc_curriculum_multicase\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "def log(msg):\n",
        "    print(msg, flush=True)\n",
        "    with open(os.path.join(OUTDIR,\"run_log.txt\"),\"a\") as f: f.write(msg+\"\\n\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_default_dtype(torch.float32)\n",
        "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
        "log(f\"Device: {device}\")\n",
        "\n",
        "# ================== Grid / Physics ==================\n",
        "NX, NY = 64, 64\n",
        "LX, LY = 1.5e6, 7.5e5\n",
        "dx, dy = LX/NX, LY/NY\n",
        "GRAV, H0   = 9.81, 100.0\n",
        "nu      = 70.0           # slightly higher viscosity for stability\n",
        "f0      = 1e-4\n",
        "beta    = 2e-11\n",
        "\n",
        "HOURS      = 6\n",
        "DT_SEC     = 2.0\n",
        "SEC_PER_HR = 3600.0\n",
        "NT         = int(HOURS*SEC_PER_HR/DT_SEC)\n",
        "\n",
        "SNAP_T     = [int(k*SEC_PER_HR/DT_SEC) for k in range(HOURS+1)]\n",
        "ANCHOR_HRS = [float(k) for k in range(HOURS+1)]     # 0..6\n",
        "MID_HRS    = [k+0.5 for k in range(HOURS)]          # 0.5..5.5\n",
        "COLL_HRS   = sorted(set(ANCHOR_HRS + MID_HRS))      # 13 times\n",
        "\n",
        "# coordinates\n",
        "x  = torch.linspace(0, LX, NX,    device=device)    # centers x\n",
        "y  = torch.linspace(0, LY, NY,    device=device)    # centers y\n",
        "xu = torch.linspace(0, LX, NX+1,  device=device)    # u x-faces (periodic)\n",
        "yv = torch.linspace(0, LY, NY+1,  device=device)    # v y-faces (walls)\n",
        "\n",
        "# ================== Helpers ==================\n",
        "def assert_shape(t, shape, name):\n",
        "    if tuple(t.shape) != tuple(shape):\n",
        "        raise RuntimeError(f\"{name} shape {tuple(t.shape)} != {tuple(shape)}\")\n",
        "\n",
        "# ----- 2D BCs (for operators/truth) -----\n",
        "def bc_uv_2d(h,u,v):\n",
        "    if h.shape[0] > 1:\n",
        "        h[0 , :] = h[1 , :]\n",
        "        h[-1, :] = h[-2, :]\n",
        "    h[:, 0]  = h[:, -2]\n",
        "    h[:, -1] = h[:, 1]\n",
        "\n",
        "    if u.shape[0] > 1:\n",
        "        u[0 , :] = u[1 , :]\n",
        "        u[-1, :] = u[-2, :]\n",
        "    u[:, 0]  = u[:, -2]\n",
        "    u[:, -1] = u[:, 1]\n",
        "\n",
        "    v[:, 0]  = v[:, -1]\n",
        "    v[0 , :] = 0.0\n",
        "    v[-1, :] = 0.0\n",
        "    return h,u,v\n",
        "\n",
        "# ----- Batched BCs (B,...) used in NN forward -----\n",
        "def bc_uv_B(h,u,v):\n",
        "    B, NYh, NXh   = h.shape\n",
        "    Bu, NYu, NXu1 = u.shape\n",
        "    Bv, NYp1, NXv = v.shape\n",
        "    assert B==Bu==Bv, \"batch mismatch in bc_uv_B\"\n",
        "    assert NXh==NXv and NXu1==NXh+1 and NYu==NYh and NYp1==NYh+1, \"grid mismatch in bc_uv_B\"\n",
        "\n",
        "    if NYh > 1:\n",
        "        h[:, 0 , :] = h[:, 1 , :]\n",
        "        h[:, -1, :] = h[:, -2, :]\n",
        "    h[:, :,  0] = h[:, :, -2]\n",
        "    h[:, :, -1] = h[:, :,  1]\n",
        "\n",
        "    if NYu > 1:\n",
        "        u[:, 0 , :] = u[:, 1 , :]\n",
        "        u[:, -1, :] = u[:, -2, :]\n",
        "    u[:, :,  0] = u[:, :, -2]\n",
        "    u[:, :, -1] = u[:, :,  1]\n",
        "\n",
        "    v[:, :, 0]  = v[:, :, -1]\n",
        "    v[:, 0 , :] = 0.0\n",
        "    v[:, -1, :] = 0.0\n",
        "    return h,u,v\n",
        "\n",
        "# ----- C-grid ops -----\n",
        "def periodic_pad_x_center(f):   return torch.cat([f[:,-1:], f, f[:,0:1]], dim=1)\n",
        "def ddx_u_to_c(u):              return (u[:,1:] - u[:,:-1]) / dx\n",
        "def ddy_v_to_c(v):              return (v[1:,:] - v[:-1,:]) / dy\n",
        "def grad_h_to_u(h):             return (periodic_pad_x_center(h)[:,1:] - periodic_pad_x_center(h)[:,:-1]) / dx\n",
        "def grad_h_to_v(h):\n",
        "    hp = torch.cat([h[0:1,:], h, h[-1:,:]], dim=0)\n",
        "    return (hp[1:,:] - hp[:-1,:]) / dy\n",
        "def avg_c_to_u(f):              return 0.5*(periodic_pad_x_center(f)[:,:-1] + periodic_pad_x_center(f)[:,1:])\n",
        "def avg_c_to_v(f):\n",
        "    fp = torch.cat([f[0:1,:], f, f[-1:,:]], dim=0)\n",
        "    return 0.5*(fp[:-1,:] + fp[1:,:])\n",
        "def avg_u_to_c(u):              return 0.5*(u[:,:-1] + u[:,1:])\n",
        "def avg_v_to_c(v):              return 0.5*(v[:-1,:] + v[1:,:])\n",
        "def ddx_u(u):                   return (torch.roll(u,-1,1) - torch.roll(u,1,1)) / (2*dx)\n",
        "def ddy_u(u):                   return (torch.roll(u,-1,0) - torch.roll(u,1,0)) / (2*dy)\n",
        "def ddx_v(v):                   return (torch.roll(v,-1,1) - torch.roll(v,1,1)) / (2*dx)\n",
        "def ddy_v(v):                   return (torch.roll(v,-1,0) - torch.roll(v,1,0)) / (2*dy)\n",
        "def lap_u(u):\n",
        "    uxx = (torch.roll(u,-1,1) - 2*u + torch.roll(u,1,1))/(dx*dx)\n",
        "    uyy = (torch.roll(u,-1,0) - 2*u + torch.roll(u,1,0))/(dy*dy)\n",
        "    return uxx + uyy\n",
        "def lap_v(v):\n",
        "    vxx = (torch.roll(v,-1,1) - 2*v + torch.roll(v,1,1))/(dx*dx)\n",
        "    vyy = (torch.roll(v,-1,0) - 2*v + torch.roll(v,1,0))/(dy*dy)\n",
        "    return vxx + vyy\n",
        "def coriolis_on_u_from_v(v):\n",
        "    v_at_c = avg_v_to_c(v); v_at_u = avg_c_to_u(v_at_c)\n",
        "    fy_u   = (f0 + beta*(y - LY/2)).view(NY,1).expand(NY,NX+1)\n",
        "    return -fy_u * v_at_u\n",
        "def coriolis_on_v_from_u(u):\n",
        "    u_at_c = avg_u_to_c(u); u_at_v = avg_c_to_v(u_at_c)\n",
        "    fy_v   = (f0 + beta*(yv - LY/2)).view(NY+1,1).expand(NY+1,NX)\n",
        "    return  fy_v * u_at_v\n",
        "\n",
        "# ----- Nonlinear SWE RHS -----\n",
        "def rhs_nl(h,u,v):\n",
        "    h,u,v = bc_uv_2d(h,u,v)\n",
        "    H = H0 + h\n",
        "    hu_u = u * avg_c_to_u(H)\n",
        "    hv_v = v * avg_c_to_v(H)\n",
        "    divF = ddx_u_to_c(hu_u) + ddy_v_to_c(hv_v)\n",
        "    Hx_u = grad_h_to_u(H)\n",
        "    Hy_v = grad_h_to_v(H)\n",
        "    v_at_u = avg_c_to_u(avg_v_to_c(v))\n",
        "    u_at_v = avg_c_to_v(avg_u_to_c(u))\n",
        "    adv_u = u*ddx_u(u) + v_at_u*ddy_u(u)\n",
        "    adv_v = u_at_v*ddx_v(v) + v*ddy_v(v)\n",
        "    visc_u = nu*lap_u(u)\n",
        "    visc_v = nu*lap_v(v)\n",
        "    h_t = -divF\n",
        "    u_t = -GRAV*Hx_u - adv_u + visc_u + coriolis_on_u_from_v(v)\n",
        "    v_t = -GRAV*Hy_v - adv_v + visc_v + coriolis_on_v_from_u(u)\n",
        "    return bc_uv_2d(h_t,u_t,v_t)\n",
        "\n",
        "# ----- RK2 truth generator -----\n",
        "def rk2_step(h,u,v,dt):\n",
        "    k1h,k1u,k1v = rhs_nl(h,u,v)\n",
        "    h1 = h + dt*k1h; u1 = u + dt*k1u; v1 = v + dt*k1v\n",
        "    k2h,k2u,k2v = rhs_nl(h1,u1,v1)\n",
        "    h_new = h + 0.5*dt*(k1h+k2h)\n",
        "    u_new = u + 0.5*dt*(k1u+k2u)\n",
        "    v_new = v + 0.5*dt*(k1v+k2v)\n",
        "    return bc_uv_2d(h_new,u_new,v_new)\n",
        "\n",
        "# ================== Multi-case ICs ==================\n",
        "def rand_ic():\n",
        "    # random sum of Gaussian bumps (2–4), amplitudes varied\n",
        "    Yc, Xc = torch.meshgrid(y, x, indexing='ij')\n",
        "    nb = np.random.randint(2,5)\n",
        "    h = torch.zeros_like(Yc, device=device)\n",
        "    for _ in range(nb):\n",
        "        cx = float(np.random.uniform(0.2,0.8) * LX)\n",
        "        cy = float(np.random.uniform(0.2,0.8) * LY)\n",
        "        sx = float(np.random.uniform(0.08,0.18) * LX)\n",
        "        sy = float(np.random.uniform(0.08,0.20) * LY)\n",
        "        amp= float(np.random.uniform(0.3,0.8))\n",
        "        h += amp*torch.exp(-(((Xc-cx)**2)/(2*sx**2) + ((Yc-cy)**2)/(2*sy**2)))\n",
        "    # small random geostrophic-like swirl (kept tiny)\n",
        "    u = torch.zeros(NY, NX+1, device=device)\n",
        "    v = torch.zeros(NY+1, NX, device=device)\n",
        "    return bc_uv_2d(h,u,v)\n",
        "\n",
        "@torch.no_grad()\n",
        "def gen_truth_from_ic(h0,u0,v0):\n",
        "    h,u,v = h0.clone(),u0.clone(),v0.clone()\n",
        "    outs=[(h.clone(),u.clone(),v.clone())]\n",
        "    tnext=1\n",
        "    for n in range(1,NT+1):\n",
        "        h,u,v = rk2_step(h,u,v,DT_SEC)\n",
        "        if tnext < len(SNAP_T) and n == SNAP_T[tnext]:\n",
        "            outs.append((h.clone(),u.clone(),v.clone()))\n",
        "            tnext += 1\n",
        "            if tnext >= len(SNAP_T): break\n",
        "    return outs\n",
        "\n",
        "N_CASES = 6   # increase this for a larger training sample\n",
        "log(f\"Generating {N_CASES} truth cases...\")\n",
        "cases_truth = []\n",
        "for c in range(N_CASES):\n",
        "    h0,u0,v0 = rand_ic()\n",
        "    traj = gen_truth_from_ic(h0,u0,v0)\n",
        "    cases_truth.append({\n",
        "        \"h\": [t[0].unsqueeze(0) for t in traj],  # (1,NY,NX)\n",
        "        \"u\": [t[1].unsqueeze(0) for t in traj],  # (1,NY,NX+1)\n",
        "        \"v\": [t[2].unsqueeze(0) for t in traj],  # (1,NY+1,NX)\n",
        "        \"ic\": (h0.unsqueeze(0), u0.unsqueeze(0), v0.unsqueeze(0))\n",
        "    })\n",
        "log(\"Truth dataset generated.\")\n",
        "\n",
        "# ================== Normalization over whole dataset ==================\n",
        "with torch.no_grad():\n",
        "    h_cat = torch.cat([torch.cat([c[\"h\"][k].reshape(1,-1) for k in range(HOURS+1)], dim=1) for c in cases_truth], dim=1)\n",
        "    u_cat = torch.cat([torch.cat([c[\"u\"][k].reshape(1,-1) for k in range(HOURS+1)], dim=1) for c in cases_truth], dim=1)\n",
        "    v_cat = torch.cat([torch.cat([c[\"v\"][k].reshape(1,-1) for k in range(HOURS+1)], dim=1) for c in cases_truth], dim=1)\n",
        "    mh, sh = float(h_cat.mean()), float(h_cat.std()+1e-6)\n",
        "    mu, su = float(u_cat.mean()), float(u_cat.std()+1e-6)\n",
        "    mv, sv = float(v_cat.mean()), float(v_cat.std()+1e-6)\n",
        "\n",
        "def znorm(x, m, s): return (x - m) / s\n",
        "def zdenorm(x, m, s): return x * s + m\n",
        "\n",
        "# ================== Model ==================\n",
        "class TimeFourier(nn.Module):\n",
        "    def __init__(self, nf=8, scale=1.0):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"k\", torch.arange(1,nf+1).float())\n",
        "        self.scale=scale; self.dim=2*nf\n",
        "    def forward(self, t_hr):\n",
        "        t=t_hr*self.scale\n",
        "        s=torch.sin(self.k[None,:]*t); c=torch.cos(self.k[None,:]*t)\n",
        "        return torch.cat([s,c], dim=-1)  # (1,2nf)\n",
        "\n",
        "def make_xy_feats():\n",
        "    Yc,Xc = torch.meshgrid(y, x, indexing='ij')            # centers\n",
        "    Yu,Xu = torch.meshgrid(y, xu, indexing='ij')           # u grid\n",
        "    Yv,Xv = torch.meshgrid(yv, x, indexing='ij')           # v grid\n",
        "    xc = (Xc/LX*2-1).unsqueeze(0).unsqueeze(0)             # (1,1,NY,NX)\n",
        "    yc = (Yc/LY*2-1).unsqueeze(0).unsqueeze(0)\n",
        "    xu_ = (Xu/LX*2-1).unsqueeze(0).unsqueeze(0)            # (1,1,NY,NX+1)\n",
        "    yu_ = (Yu/LY*2-1).unsqueeze(0).unsqueeze(0)\n",
        "    xv_ = (Xv/LX*2-1).unsqueeze(0).unsqueeze(0)            # (1,1,NY+1,NX)\n",
        "    yv_ = (Yv/LY*2-1).unsqueeze(0).unsqueeze(0)\n",
        "    return xc,yc,xu_,yu_,xv_,yv_\n",
        "XC,YC,XU,YU,XV,YV = make_xy_feats()\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, c_in, c_out, hidden=160, depth=5):\n",
        "        super().__init__()\n",
        "        L=[nn.Conv2d(c_in, hidden, 3, padding=1), nn.GELU()]\n",
        "        for _ in range(depth-1): L += [nn.Conv2d(hidden, hidden, 3, padding=1), nn.GELU()]\n",
        "        L += [nn.Conv2d(hidden, c_out, 3, padding=1)]\n",
        "        self.net = nn.Sequential(*L)\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class STUNet(nn.Module):\n",
        "    \"\"\" Predict absolute (h,u,v) at time t from IC and (x,y,t) features \"\"\"\n",
        "    def __init__(self, nf_t=8):\n",
        "        super().__init__()\n",
        "        self.tfe = TimeFourier(nf=nf_t, scale=1.0)\n",
        "        tdim = self.tfe.dim\n",
        "        self.h_trunk = Block(c_in=1+1+tdim, c_out=1)  # [h0_norm, pos-sum, t_fourier]\n",
        "        self.u_trunk = Block(c_in=1+1+tdim, c_out=1)\n",
        "        self.v_trunk = Block(c_in=1+1+tdim, c_out=1)\n",
        "        self.logsig_h = nn.Parameter(torch.tensor(0.0))\n",
        "        self.logsig_u = nn.Parameter(torch.tensor(0.0))\n",
        "        self.logsig_v = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, h0,u0,v0, t_hr):\n",
        "        tf = self.tfe(t_hr)  # (1,tdim)\n",
        "        B=1\n",
        "        tf_c = tf.view(B,-1,1,1).expand(B,-1,NY,NX)\n",
        "        tf_u = tf.view(B,-1,1,1).expand(B,-1,NY,NX+1)\n",
        "        tf_v = tf.view(B,-1,1,1).expand(B,-1,NY+1,NX)\n",
        "\n",
        "        h0n = znorm(h0, mh, sh)                 # (1,NY,NX)\n",
        "        u0n = znorm(u0, mu, su)                 # (1,NY,NX+1)\n",
        "        v0n = znorm(v0, mv, sv)                 # (1,NY+1,NX)\n",
        "\n",
        "        hc_in = torch.cat([h0n.unsqueeze(1), (XC+YC), tf_c], dim=1)\n",
        "        hu_in = torch.cat([u0n.unsqueeze(1), (XU+YU), tf_u], dim=1)\n",
        "        hv_in = torch.cat([v0n.unsqueeze(1), (XV+YV), tf_v], dim=1)\n",
        "\n",
        "        hn = self.h_trunk(hc_in).squeeze(1)     # (1,NY,NX)\n",
        "        un = self.u_trunk(hu_in).squeeze(1)     # (1,NY,NX+1)\n",
        "        vn = self.v_trunk(hv_in).squeeze(1)     # (1,NY+1,NX)\n",
        "\n",
        "        h = zdenorm(hn, mh, sh)\n",
        "        u = zdenorm(un, mu, su)\n",
        "        v = zdenorm(vn, mv, sv)\n",
        "\n",
        "        # Soft clamps to avoid JVP blow-ups\n",
        "        h = torch.clamp(h, -2.0, 2.0)\n",
        "        u = torch.clamp(u, -25.0, 25.0)\n",
        "        v = torch.clamp(v, -25.0, 25.0)\n",
        "\n",
        "        h,u,v = bc_uv_B(h,u,v)\n",
        "        assert_shape(h[0], (NY,NX),   \"model h\")\n",
        "        assert_shape(u[0], (NY,NX+1), \"model u\")\n",
        "        assert_shape(v[0], (NY+1,NX), \"model v\")\n",
        "        return h,u,v\n",
        "\n",
        "model = STUNet().to(device)\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=800, eta_min=1e-5)\n",
        "\n",
        "# ================== AD-PINN residuals ==================\n",
        "T_scale = HOURS*SEC_PER_HR\n",
        "eta_scale = max(float(h_cat.std().cpu()), 1e-6)\n",
        "u_scale   = max(float(u_cat.std().cpu()), 1e-6)\n",
        "v_scale   = max(float(v_cat.std().cpu()), 1e-6)\n",
        "\n",
        "def pack_flat(h,u,v):\n",
        "    return torch.cat([h.reshape(-1), u.reshape(-1), v.reshape(-1)], dim=0)\n",
        "def unpack_flat(yflat):\n",
        "    n_h = NY*NX\n",
        "    n_u = NY*(NX+1)\n",
        "    n_v = (NY+1)*NX\n",
        "    h = yflat[:n_h].view(1,NY,NX)\n",
        "    u = yflat[n_h:n_h+n_u].view(1,NY,NX+1)\n",
        "    v = yflat[n_h+n_u:].view(1,NY+1,NX)\n",
        "    return h,u,v\n",
        "def f_time_only(t_hr_scalar, h0,u0,v0):\n",
        "    h,u,v = model(h0,u0,v0, t_hr_scalar)\n",
        "    return pack_flat(h,u,v)\n",
        "\n",
        "def residuals_AD(h0,u0,v0, t_hr_scalar):\n",
        "    t = torch.tensor(float(t_hr_scalar), device=device, dtype=torch.float32, requires_grad=True)\n",
        "    y, dy_dt_hr = torch.autograd.functional.jvp(\n",
        "        lambda tt: f_time_only(tt, h0,u0,v0),\n",
        "        (t,), (torch.ones_like(t),), create_graph=True\n",
        "    )\n",
        "    h, u, v = unpack_flat(y)               # (1,...)\n",
        "    dhdt, dudt, dvdt = unpack_flat(dy_dt_hr / SEC_PER_HR)\n",
        "    # drop batch for FD ops\n",
        "    h, u, v = h[0], u[0], v[0]\n",
        "    dhdt, dudt, dvdt = dhdt[0], dudt[0], dvdt[0]\n",
        "\n",
        "    H = H0 + h\n",
        "    hu_u = u * avg_c_to_u(H)\n",
        "    hv_v = v * avg_c_to_v(H)\n",
        "    cont = dhdt + ddx_u_to_c(hu_u) + ddy_v_to_c(hv_v)\n",
        "\n",
        "    Hx_u = grad_h_to_u(H)\n",
        "    Hy_v = grad_h_to_v(H)\n",
        "    v_at_u = avg_c_to_u(avg_v_to_c(v))\n",
        "    u_at_v = avg_c_to_v(avg_u_to_c(u))\n",
        "    adv_u = u*ddx_u(u) + v_at_u*ddy_u(u)\n",
        "    adv_v = u_at_v*ddx_v(v) + v*ddy_v(v)\n",
        "    R_u = dudt + GRAV*Hx_u + adv_u - nu*lap_u(u) - coriolis_on_u_from_v(v)\n",
        "    R_v = dvdt + GRAV*Hy_v + adv_v - nu*lap_v(v) + coriolis_on_v_from_u(u)\n",
        "\n",
        "    Rh = cont / (eta_scale / T_scale)\n",
        "    Ru = R_u  / (u_scale   / T_scale)\n",
        "    Rv = R_v  / (v_scale   / T_scale)\n",
        "    return Rh, Ru, Rv\n",
        "\n",
        "# ================== Training (curriculum + multi-case) ==================\n",
        "EPOCHS_DATA  = 250   # longer pure-data pretrain (stability)\n",
        "EPOCHS_JOINT = 400\n",
        "TOTAL_EPOCHS = EPOCHS_DATA + EPOCHS_JOINT\n",
        "\n",
        "def lambda_phys(ep):\n",
        "    if ep <= EPOCHS_DATA: return 0.0\n",
        "    k = ep - EPOCHS_DATA\n",
        "    return min(0.02, 1e-3 + 5e-5*k)  # gentle ramp up to 0.02\n",
        "\n",
        "# midpoints weighting slightly lower\n",
        "def colloc_weight(t_hr): return 1.0 if t_hr in ANCHOR_HRS else 0.7\n",
        "\n",
        "# loss balancing across anchors (later anchors slightly heavier)\n",
        "anchor_w = {k: 1.0 + 0.10*k for k in range(HOURS+1)}\n",
        "w_sum = sum(anchor_w.values())\n",
        "\n",
        "def safe_mean(x):\n",
        "    x = torch.where(torch.isfinite(x), x, torch.zeros_like(x))\n",
        "    return x.mean()\n",
        "\n",
        "logs=[]\n",
        "for ep in range(1, TOTAL_EPOCHS+1):\n",
        "    opt.zero_grad()\n",
        "    L_data = torch.tensor(0.0, device=device)\n",
        "    L_phys = torch.tensor(0.0, device=device)\n",
        "    lam    = lambda_phys(ep)\n",
        "\n",
        "    # === DATA: average over all cases and anchors ===\n",
        "    for ci in range(N_CASES):\n",
        "        h0,u0,v0 = cases_truth[ci][\"ic\"]\n",
        "        for t_hr in ANCHOR_HRS:\n",
        "            hp,up,vp = model(h0,u0,v0, torch.tensor(t_hr,device=device))\n",
        "            k = int(round(t_hr))\n",
        "            w = anchor_w.get(k,1.0)\n",
        "            Ld = (F.mse_loss(znorm(hp,mh,sh), znorm(cases_truth[ci][\"h\"][k],mh,sh)) +\n",
        "                  F.mse_loss(znorm(up,mu,su), znorm(cases_truth[ci][\"u\"][k],mu,su)) +\n",
        "                  F.mse_loss(znorm(vp,mv,sv), znorm(cases_truth[ci][\"v\"][k],mv,sv)))\n",
        "            L_data += w*Ld\n",
        "    L_data = L_data / (w_sum * N_CASES)\n",
        "\n",
        "    # === PHYSICS: over all cases and collocation times ===\n",
        "    if lam > 0:\n",
        "        for ci in range(N_CASES):\n",
        "            h0,u0,v0 = cases_truth[ci][\"ic\"]\n",
        "            for t_hr in COLL_HRS:\n",
        "                Rh, Ru, Rv = residuals_AD(h0,u0,v0, t_hr)\n",
        "                # clamp logsig to avoid exp overflow/underflow\n",
        "                sh = model.logsig_h.clamp(-2.0, 2.0)\n",
        "                su = model.logsig_u.clamp(-2.0, 2.0)\n",
        "                sv = model.logsig_v.clamp(-2.0, 2.0)\n",
        "                Lp  = 0.5*(torch.exp(-2*sh)*safe_mean(Rh.pow(2)) + 2*sh)\n",
        "                Lp += 0.5*(torch.exp(-2*su)*safe_mean(Ru.pow(2)) + 2*su)\n",
        "                Lp += 0.5*(torch.exp(-2*sv)*safe_mean(Rv.pow(2)) + 2*sv)\n",
        "                L_phys += colloc_weight(t_hr)*Lp\n",
        "        L_phys = L_phys / (N_CASES * (len(ANCHOR_HRS) + 0.7*len(MID_HRS)))\n",
        "\n",
        "    L = L_data + lam*L_phys\n",
        "\n",
        "    if not torch.isfinite(L):\n",
        "        log(f\"[NaN GUARD] Non-finite loss at epoch {ep}: L={L} data={L_data} phys={L_phys} λ={lam}\")\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        # reduce LR proactively\n",
        "        for g in opt.param_groups: g['lr'] = max(g['lr']*0.5, 1e-5)\n",
        "        continue\n",
        "\n",
        "    L.backward()\n",
        "    # tighter clip when physics > 0\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5 if lam>0 else 1.0)\n",
        "    # optional LR tweak when physics starts\n",
        "    if lam>0 and sched.get_last_lr()[0] > 2e-4:\n",
        "        for g in opt.param_groups: g['lr'] = 2e-4\n",
        "    opt.step(); sched.step()\n",
        "\n",
        "    logs.append([ep, float(L.item()), float(L_data.item()), float(L_phys.item()), float(lam),\n",
        "                 float(model.logsig_h.item()), float(model.logsig_u.item()), float(model.logsig_v.item()),\n",
        "                 float(sched.get_last_lr()[0])])\n",
        "    if ep%50==0 or ep in (1,EPOCHS_DATA, TOTAL_EPOCHS):\n",
        "        log(f\"[{ep:04d}] L={L:.3e} data={L_data:.3e} phys={L_phys:.3e} λ={lam:.3f} \"\n",
        "            f\"logsig=({model.logsig_h.item():.2f},{model.logsig_u.item():.2f},{model.logsig_v.item():.2f}) \"\n",
        "            f\"lr={sched.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "with open(os.path.join(OUTDIR,\"losses_train.csv\"),\"w\",newline=\"\") as f:\n",
        "    w=csv.writer(f); w.writerow([\"epoch\",\"total\",\"data\",\"phys\",\"lambda\",\"logsig_h\",\"logsig_u\",\"logsig_v\",\"lr\"]); w.writerows(logs)\n",
        "\n",
        "# ================== Evaluation & Plots (guarded) ==================\n",
        "def guard_plot(path, fn):\n",
        "    try:\n",
        "        fn(); plt.savefig(path,dpi=140); plt.close(); log(f\"Saved: {path}\")\n",
        "    except Exception as e:\n",
        "        log(f\"[PLOT ERROR] {path}: {e}\")\n",
        "        with open(os.path.join(OUTDIR,\"plot_errors.txt\"),\"a\") as f:\n",
        "            f.write(f\"{path}: {e}\\n{traceback.format_exc()}\\n\")\n",
        "\n",
        "try:\n",
        "    @torch.no_grad()\n",
        "    def predict_case_hour(ci, th):\n",
        "        h0,u0,v0 = cases_truth[ci][\"ic\"]\n",
        "        return model(h0,u0,v0, torch.tensor(float(th),device=device))\n",
        "\n",
        "    def rmse(a,b): return float(torch.sqrt(((a-b)**2).mean()).item())\n",
        "\n",
        "    # RMSE by hour averaged across cases\n",
        "    hrs = np.arange(HOURS+1, dtype=float)\n",
        "    RM_h = np.zeros_like(hrs, dtype=float)\n",
        "    RM_u = np.zeros_like(hrs, dtype=float)\n",
        "    RM_v = np.zeros_like(hrs, dtype=float)\n",
        "\n",
        "    # collect predictions for case 0 to plot maps\n",
        "    pred0 = []\n",
        "    for k in range(HOURS+1):\n",
        "        hp0,up0,vp0 = predict_case_hour(0, k)\n",
        "        pred0.append((hp0,up0,vp0))\n",
        "\n",
        "    for k in range(HOURS+1):\n",
        "        rh,ru,rv = 0.0,0.0,0.0\n",
        "        for ci in range(N_CASES):\n",
        "            hp,up,vp = predict_case_hour(ci, k)\n",
        "            rh += rmse(hp, cases_truth[ci][\"h\"][k])\n",
        "            ru += rmse(up, cases_truth[ci][\"u\"][k])\n",
        "            rv += rmse(vp, cases_truth[ci][\"v\"][k])\n",
        "        RM_h[k] = rh / N_CASES\n",
        "        RM_u[k] = ru / N_CASES\n",
        "        RM_v[k] = rv / N_CASES\n",
        "\n",
        "    # energy on case 0\n",
        "    def energy(hb, ub, vb):\n",
        "        hb = hb[0]; ub = ub[0]; vb = vb[0]\n",
        "        assert hb.shape == (NY, NX)\n",
        "        assert ub.shape == (NY, NX+1)\n",
        "        assert vb.shape == (NY+1, NX)\n",
        "        uc = 0.5*(ub[:, :-1] + ub[:, 1:])\n",
        "        vc = 0.5*(vb[:-1, :] + vb[1:, :])\n",
        "        KE = 0.5*H0*(uc*uc + vc*vc)\n",
        "        PE = 0.5*GRAV*(hb*hb)\n",
        "        TE = KE + PE\n",
        "        return float(KE.mean()), float(PE.mean()), float(TE.mean())\n",
        "\n",
        "    KEs, PEs, TEs = [],[],[]\n",
        "    for k in range(HOURS+1):\n",
        "        ke,pe,te = energy(*pred0[k]); KEs.append(ke); PEs.append(pe); TEs.append(te)\n",
        "\n",
        "    # Save CSVs\n",
        "    with open(os.path.join(OUTDIR,\"rmse_vs_hour_avg.csv\"),\"w\",newline=\"\") as f:\n",
        "        w=csv.writer(f); w.writerow([\"hour\",\"RMSE_h_mean\",\"RMSE_u_mean\",\"RMSE_v_mean\"])\n",
        "        for h,a,b,c in zip(hrs,RM_h,RM_u,RM_v): w.writerow([float(h),float(a),float(b),float(c)])\n",
        "    with open(os.path.join(OUTDIR,\"energy_vs_hour_case0.csv\"),\"w\",newline=\"\") as f:\n",
        "        w=csv.writer(f); w.writerow([\"hour\",\"KE\",\"PE\",\"TE\"])\n",
        "        for h,ke,pe,te in zip(hrs,KEs,PEs,TEs): w.writerow([float(h),float(ke),float(pe),float(te)])\n",
        "\n",
        "    # Plots\n",
        "    def imshow(ax, field, title):\n",
        "        im=ax.imshow(field, origin='lower', interpolation='nearest', cmap='viridis')\n",
        "        ax.set_title(title, fontsize=10); ax.set_xticks([]); ax.set_yticks([])\n",
        "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "    hT_true = cases_truth[0][\"h\"][-1][0].cpu().numpy()\n",
        "    uT_true = cases_truth[0][\"u\"][-1][0].cpu().numpy()\n",
        "    vT_true = cases_truth[0][\"v\"][-1][0].cpu().numpy()\n",
        "    hT_pred = pred0[-1][0][0].cpu().numpy()\n",
        "    uT_pred = pred0[-1][1][0].cpu().numpy()\n",
        "    vT_pred = pred0[-1][2][0].cpu().numpy()\n",
        "\n",
        "    guard_plot(os.path.join(OUTDIR,\"maps_h_u_v_T6_case0.png\"), lambda: (\n",
        "        plt.figure(figsize=(12,8)),\n",
        "        imshow(plt.subplot(3,3,1), hT_true, \"h True (case0)\"),\n",
        "        imshow(plt.subplot(3,3,2), hT_pred, \"h Pred (case0)\"),\n",
        "        imshow(plt.subplot(3,3,3), hT_pred-hT_true, \"h Diff\"),\n",
        "        imshow(plt.subplot(3,3,4), uT_true, \"u True (case0)\"),\n",
        "        imshow(plt.subplot(3,3,5), uT_pred, \"u Pred (case0)\"),\n",
        "        imshow(plt.subplot(3,3,6), uT_pred-uT_true, \"u Diff\"),\n",
        "        imshow(plt.subplot(3,3,7), vT_true, \"v True (case0)\"),\n",
        "        imshow(plt.subplot(3,3,8), vT_pred, \"v Pred (case0)\"),\n",
        "        imshow(plt.subplot(3,3,9), vT_pred-vT_true, \"v Diff\"),\n",
        "        plt.tight_layout()\n",
        "    ))\n",
        "\n",
        "    guard_plot(os.path.join(OUTDIR,\"rmse_vs_hour_avg.png\"), lambda: (\n",
        "        plt.figure(figsize=(7,5)),\n",
        "        plt.plot(hrs, RM_h, label='h (avg over cases)'),\n",
        "        plt.plot(hrs, RM_u, label='u (avg)'),\n",
        "        plt.plot(hrs, RM_v, label='v (avg)'),\n",
        "        plt.xlabel(\"hour\"), plt.ylabel(\"RMSE\"), plt.title(\"RMSE vs hour (mean over cases)\"),\n",
        "        plt.legend(), plt.tight_layout()\n",
        "    ))\n",
        "\n",
        "    guard_plot(os.path.join(OUTDIR,\"energy_vs_hour_case0.png\"), lambda: (\n",
        "        plt.figure(figsize=(7,5)),\n",
        "        plt.plot(hrs,KEs,label=\"KE\"), plt.plot(hrs,PEs,label=\"PE\"), plt.plot(hrs,TEs,label=\"TE\"),\n",
        "        plt.xlabel(\"hour\"), plt.ylabel(\"Energy\"), plt.title(\"Energy vs hour (case 0)\"),\n",
        "        plt.legend(), plt.tight_layout()\n",
        "    ))\n",
        "\n",
        "    log(\"SUCCESS\")\n",
        "    sys.exit(0)\n",
        "\n",
        "except Exception as e:\n",
        "    with open(os.path.join(OUTDIR,\"post_training_error.txt\"),\"w\") as f:\n",
        "        f.write(str(e) + \"\\n\" + traceback.format_exc())\n",
        "    log(f\"POST_TRAINING_FAILURE: {e}\")\n",
        "    sys.exit(1)\n"
      ],
      "metadata": {
        "id": "trCoVTFXqyR5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "e4bafd70-b201-4bfa-c3aa-f6efea181ec6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Generating 6 truth cases...\n",
            "Truth dataset generated.\n",
            "[0001] L=3.036e+00 data=3.036e+00 phys=0.000e+00 λ=0.000 logsig=(0.00,0.00,0.00) lr=3.00e-04\n",
            "[0050] L=2.405e+00 data=2.405e+00 phys=0.000e+00 λ=0.000 logsig=(0.00,0.00,0.00) lr=2.97e-04\n",
            "[0100] L=2.226e+00 data=2.226e+00 phys=0.000e+00 λ=0.000 logsig=(0.00,0.00,0.00) lr=2.89e-04\n",
            "[0150] L=2.129e+00 data=2.129e+00 phys=0.000e+00 λ=0.000 logsig=(0.00,0.00,0.00) lr=2.76e-04\n",
            "[0200] L=2.062e+00 data=2.062e+00 phys=0.000e+00 λ=0.000 logsig=(0.00,0.00,0.00) lr=2.58e-04\n",
            "[0250] L=2.035e+00 data=2.035e+00 phys=0.000e+00 λ=0.000 logsig=(0.00,0.00,0.00) lr=2.36e-04\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 47453 has 14.72 GiB memory in use. Of the allocated memory 13.30 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3935412316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcases_truth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mci\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt_hr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCOLL_HRS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 \u001b[0mRh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresiduals_AD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_hr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m                 \u001b[0;31m# clamp logsig to avoid exp overflow/underflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0msh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsig_h\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3935412316.py\u001b[0m in \u001b[0;36mresiduals_AD\u001b[0;34m(h0, u0, v0, t_hr_scalar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresiduals_AD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_hr_scalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_hr_scalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     y, dy_dt_hr = torch.autograd.functional.jvp(\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf_time_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjvp\u001b[0;34m(func, inputs, v, create_graph, strict)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             grad_res = _autograd_grad(\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0mgrad_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         return torch.autograd.grad(\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mnew_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    501\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 47453 has 14.72 GiB memory in use. Of the allocated memory 13.30 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UW0IW6BBnvqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wig0hCdCqyVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgo_Hv0So2HV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}